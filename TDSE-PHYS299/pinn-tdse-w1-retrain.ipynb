{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# use float64 for higher precision in PDEs\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "DTYPE = tf.float64\n",
    "\n",
    "# declare inputs\n",
    "# w_min, w_max = tf.constant(0.75, dtype=DTYPE), tf.constant(2.0, dtype=DTYPE)\n",
    "w_choice = tf.constant(1.0, dtype=DTYPE)\n",
    "pi = tf.constant(np.pi, dtype=DTYPE)\n",
    "x_min, x_max = -pi, pi\n",
    "t_min, t_max = tf.constant(0.0, dtype=DTYPE), pi/2.0\n",
    "\n",
    "# scale inputs\n",
    "@tf.function\n",
    "def scale_inputs(input, input_min, input_max):\n",
    "    return 2.0 * (input - input_min) / (input_max - input_min) - 1.0\n",
    "\n",
    "# # renormalization\n",
    "# @tf.function\n",
    "# def normalization(input_min, input_max, N, psi_Re, psi_Im):\n",
    "#     dx = (input_max - input_min) / tf.cast(N, dtype=DTYPE)\n",
    "#     norm = tf.math.sqrt(tf.reduce_sum(psi_Re**2 + psi_Im**2) * dx)\n",
    "#     return (psi_Re/norm), (psi_Im/norm)\n",
    "\n",
    "# initial condition\n",
    "@tf.function\n",
    "def psi_init(x, w, pi):\n",
    "    u_0 = (w/pi)**0.25 * tf.math.exp(-0.5*w*x**2)\n",
    "    u_1 = (w/pi)**0.25 * tf.math.exp(-0.5*w*x**2) * x * tf.math.sqrt(2.0*w)\n",
    "    return (1.0/np.sqrt(2.0)) * (u_0 + u_1)\n",
    "\n",
    "# data generation\n",
    "N_f, N_b, N_i, N_grid = 5000, 1000, 1000, 200\n",
    "\n",
    "# interior points\n",
    "x_f = tf.random.uniform((N_f, 1), x_min, x_max, dtype=DTYPE)\n",
    "t_f = tf.random.uniform((N_f, 1), t_min, t_max, dtype=DTYPE)\n",
    "x_fs = scale_inputs(x_f, x_min, x_max)  # scaled\n",
    "t_fs = scale_inputs(t_f, t_min, t_max)  # scaled\n",
    "\n",
    "# boundary points\n",
    "x_b = tf.concat([tf.ones((N_b//2, 1), dtype=DTYPE) * x_min,\n",
    "                 tf.ones((N_b//2, 1), dtype=DTYPE) * x_max], axis=0)\n",
    "t_b = tf.random.uniform((N_b, 1), t_min, t_max, dtype=DTYPE)\n",
    "x_bs = scale_inputs(x_b, x_min, x_max)  # scaled\n",
    "t_bs = scale_inputs(t_b, t_min, t_max)  # scaled\n",
    "\n",
    "# initial points\n",
    "x_i = tf.random.uniform((N_i, 1), x_min, x_max, dtype=DTYPE)\n",
    "t_i = tf.zeros_like(x_i, dtype=DTYPE)\n",
    "x_is = scale_inputs(x_i, x_min, x_max)  # scaled\n",
    "t_is = scale_inputs(t_i, t_min, t_max)  # scaled\n",
    "\n",
    "psi_i = tf.cast(psi_init(x_i, w_choice, pi), tf.complex128)\n",
    "\n",
    "# grid for normalization penalty\n",
    "x_n = tf.linspace(x_min, x_max, N_grid)[:, None]\n",
    "x_ns = scale_inputs(x_n, x_min, x_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd679d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourier feature layer\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class FourierFeatureLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, mapping_size=512, scale=10.0, **kwargs):\n",
    "        super(FourierFeatureLayer, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.mapping_size = mapping_size\n",
    "        self.scale = scale\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.B = self.add_weight(name=\"B\",\n",
    "                                 shape=[self.input_dim, self.mapping_size],\n",
    "                                 initializer=tf.random_normal_initializer(stddev=self.scale),\n",
    "                                 trainable=False)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x_proj = tf.matmul(x, self.B)\n",
    "        return tf.concat([tf.math.sin(x_proj), tf.math.cos(x_proj)], axis=-1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"mapping_size\": self.mapping_size,\n",
    "            \"scale\": self.scale\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "# Define the Neural Network\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, input_dim=2, mapping_size=512, scale=10.0, **kwargs):\n",
    "        super(PINN, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.mapping_size = mapping_size\n",
    "        self.scale = scale\n",
    "\n",
    "        self.fourier = FourierFeatureLayer(input_dim=input_dim, mapping_size=mapping_size, scale=scale)\n",
    "        initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(512, activation='tanh', kernel_initializer=initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(512, activation='tanh', kernel_initializer=initializer)\n",
    "        self.dense3 = tf.keras.layers.Dense(512, activation='tanh', kernel_initializer=initializer)\n",
    "        self.dense4 = tf.keras.layers.Dense(512, activation='tanh', kernel_initializer=initializer)\n",
    "        self.out = tf.keras.layers.Dense(2, activation='linear', kernel_initializer=initializer) # u_Re, u_Im\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.fourier(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"mapping_size\": self.mapping_size,\n",
    "            \"scale\": self.scale\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "# We use Hartree atomic units \\hbar = m = 1\n",
    "# Define the Physics Loss\n",
    "@tf.function\n",
    "def pde_residual(model, x, t):\n",
    "    with tf.GradientTape(persistent=True) as tape2:\n",
    "        tape2.watch([x, t])\n",
    "        with tf.GradientTape(persistent=True) as tape1:\n",
    "            tape1.watch([x, t])\n",
    "            xs = scale_inputs(x, x_min, x_max)\n",
    "            ts = scale_inputs(t, t_min, t_max)\n",
    "            X = tf.concat([xs, ts], axis=1)\n",
    "            # uv = tf.clip_by_value(model(X), -1e2, 1e2) # prevent extreme values\n",
    "            uv = model(X)\n",
    "            u, v = uv[:, 0:1], uv[:, 1:2]\n",
    "        u_x = tape1.gradient(u, x) #, unconnected_gradients='zero')\n",
    "        v_x = tape1.gradient(v, x) #, unconnected_gradients='zero')\n",
    "        u_t = tape1.gradient(u, t) #, unconnected_gradients='zero')\n",
    "        v_t = tape1.gradient(v, t) #, unconnected_gradients='zero')\n",
    "    u_xx = tape2.gradient(u_x, x) #, unconnected_gradients='zero')\n",
    "    v_xx = tape2.gradient(v_x, x) #, unconnected_gradients='zero')\n",
    "    del tape1, tape2\n",
    "\n",
    "    potential = 0.5 * w_choice**2 * x**2\n",
    "\n",
    "    f_u = -v_t + 0.5*u_xx - potential*u\n",
    "    f_v = u_t + 0.5*v_xx - potential*v\n",
    "\n",
    "    return f_u, f_v\n",
    "\n",
    "@tf.function\n",
    "def norm_loss(model, x_ns, N_t):\n",
    "    t_n = tf.random.uniform((N_t, 1), t_min, t_max, dtype=DTYPE)\n",
    "    t_ns = scale_inputs(t_n, t_min, t_max)\n",
    "    norm_penalty_total = 0.0\n",
    "    for t_val in tf.unstack(t_ns, axis=0):\n",
    "        t_in = tf.ones_like(x_ns) * t_val\n",
    "        X = tf.concat([x_ns, t_in], axis=1)\n",
    "        uv = model(X)\n",
    "        dx = (x_max - x_min) / tf.cast(tf.shape(x_ns)[0], dtype=DTYPE)\n",
    "        norm = tf.reduce_sum(tf.math.abs(uv)**2) * dx\n",
    "        norm_penalty_total += tf.square(norm - 1.0)\n",
    "    norm_penalty = norm_penalty_total / tf.cast(N_t, dtype=DTYPE)\n",
    "\n",
    "    return norm_penalty\n",
    "\n",
    "@tf.function\n",
    "def loss_fn(model, interior, boundary, initial, norm_grid):\n",
    "    x_f, t_f = interior\n",
    "    x_b, t_b = boundary\n",
    "    x_i, t_i, psi_i = initial\n",
    "    x_grid, N_t = norm_grid\n",
    "\n",
    "    f_u, f_v = pde_residual(model, x_f, t_f)\n",
    "    loss_f = tf.reduce_mean(tf.square(f_u) + tf.square(f_v)) # / tf.cast(tf.shape(x_f)[0], dtype=DTYPE)\n",
    "\n",
    "    uv_b = model(tf.concat([x_b, t_b], axis=1))\n",
    "    loss_b = tf.reduce_mean(tf.square(uv_b[:, 0:1]) + tf.square(uv_b[:, 1:2])) # / tf.cast(tf.shape(x_b)[0], dtype=DTYPE)\n",
    "\n",
    "    uv_i = model(tf.concat([x_i, t_i], axis=1))\n",
    "    loss_i = tf.reduce_mean(tf.square(uv_i[:, 0:1] - tf.math.real(psi_i)) +\n",
    "                            tf.square(uv_i[:, 1:2] - tf.math.imag(psi_i))) # / tf.cast(tf.shape(x_i)[0], dtype=DTYPE)\n",
    "    \n",
    "    norm_penalty = norm_loss(model, x_grid, N_t)\n",
    "    \n",
    "    return loss_f + 100.0*loss_b + 100.0*loss_i + 100.0*norm_penalty, (loss_f, 100.0*loss_b, 100.0*loss_i, 100.0*norm_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a60369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Optimizer\n",
    "# Learning rate schedule with exponential decay\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=2000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# ADAM optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=lr_schedule,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "# Training Step Function\n",
    "@tf.function\n",
    "def train_step(model, x_f, t_f, x_b, t_b, x_i, t_i, psi_i, x_n, N_t):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, (lf, lb, li, ln) = loss_fn(model, (x_f, t_f), (x_b, t_b), (x_i, t_i, psi_i), (x_n, N_t))\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, (lf, lb, li, ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the Model\n",
    "pinn = PINN(input_dim=2, mapping_size=512, scale=10.0)\n",
    "\n",
    "# pinn = tf.keras.models.load_model('best_model_w1p1.keras', custom_objects={'PINN': PINN})\n",
    "# print(\"Model loaded successfully\")\n",
    "\n",
    "# Define the loss threshold\n",
    "loss_threshold = 1e-10\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_patience = 1000\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# saving loss values\n",
    "train_loss_results = {\n",
    "    \"total\": [],\n",
    "    \"pde\": [],\n",
    "    \"boundary\": [],\n",
    "    \"initial\": [],\n",
    "    \"norm\" : []\n",
    "}\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "best_model_path = \"best_model_w1_s1.keras\"\n",
    "\n",
    "# Training Loop until loss < loss_threshold\n",
    "epoch = 0\n",
    "while True:\n",
    "    loss, (lf, lb, li, ln) = train_step(pinn, x_f, t_f, x_bs, t_bs, x_is, t_is, psi_i, x_ns, 3)\n",
    "\n",
    "    train_loss_results[\"total\"].append(loss.numpy())\n",
    "    train_loss_results[\"pde\"].append(lf.numpy())\n",
    "    train_loss_results[\"boundary\"].append(lb.numpy())\n",
    "    train_loss_results[\"initial\"].append(li.numpy())\n",
    "    train_loss_results[\"norm\"].append(ln.numpy())\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.numpy().item():.5e} (f = {lf.numpy().item():.3e}, b = {lb.numpy().item():.3e}, i = {li.numpy().item():.3e}, n = {ln.numpy().item():.3e})\")\n",
    "\n",
    "        X = tf.concat([x_is, t_is], axis=1)\n",
    "        uv = pinn(X)\n",
    "        u, v = uv[:, 0:1], uv[:, 1:2]\n",
    "        # test_Re, test_Im = normalization(x_min, x_max, N_i, u, v)\n",
    "        density = u**2 + v**2\n",
    "\n",
    "        plt.scatter(x_i, density, label=r'$|\\Psi(x,0)|^2$ with \\omega=1.0')\n",
    "        plt.xlabel('x')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    if loss.numpy() < best_loss:\n",
    "        best_loss = loss.numpy()\n",
    "        patience_counter = 0\n",
    "        pinn.save(best_model_path)\n",
    "        print(f\"Best model saved to {best_model_path} at epoch {epoch} with loss {loss.numpy()}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(f\"Early stopping at epoch {epoch} as loss did not improve for {early_stopping_patience} epochs.\")\n",
    "        print(f\"Training stopped at epoch {epoch} with loss {loss.numpy()}\")\n",
    "        break\n",
    "    \n",
    "    if loss.numpy() < loss_threshold:\n",
    "        print(f\"Training stopped at epoch {epoch} as loss reached {loss.numpy()}\")\n",
    "        break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Convert to hours, minutes, seconds\n",
    "execution_time_formatted = str(datetime.timedelta(seconds=int(execution_time)))\n",
    "\n",
    "print(f\"Total training time: {execution_time_formatted}\")\n",
    "        \n",
    "# Plot the Loss vs. Epochs\n",
    "plt.plot(train_loss_results[\"total\"], label=\"Total Loss\")\n",
    "plt.plot(train_loss_results[\"pde\"], label=\"PDE Loss\")\n",
    "plt.plot(train_loss_results[\"boundary\"], label=\"Boundary Loss\")\n",
    "plt.plot(train_loss_results[\"initial\"], label=\"Initial Loss\")\n",
    "plt.plot(train_loss_results[\"norm\"], label=\"Norm Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss Components During Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (log scale)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# # Save original weights\n",
    "# original_weights = pinn.get_weights()\n",
    "\n",
    "# # Load the model\n",
    "# loaded_pinn = tf.keras.models.load_model(best_model_path, custom_objects={'PINN': PINN})\n",
    "\n",
    "# # Check if the weights match\n",
    "# loaded_weights = loaded_pinn.get_weights()\n",
    "\n",
    "# for orig, loaded in zip(original_weights, loaded_weights):\n",
    "#     if not np.array_equal(orig, loaded):\n",
    "#         print(\"Weights do not match after loading!\")\n",
    "#     else:\n",
    "#         print(\"Weights match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3fa8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.save('last_trained_w1_s1.keras')\n",
    "print(f\"last model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25caaa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"train_loss_results_w1_s1.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writerow([\"epoch\", \"total_loss\", \"pde_loss\", \"boundary_loss\", \"initial_loss\", \"norm_loss\"])\n",
    "    \n",
    "    # Write data row by row\n",
    "    for epoch, (total, pde, boundary, initial, norm) in enumerate(zip(\n",
    "        train_loss_results[\"total\"],\n",
    "        train_loss_results[\"pde\"],\n",
    "        train_loss_results[\"boundary\"],\n",
    "        train_loss_results[\"initial\"],\n",
    "        train_loss_results[\"norm\"]\n",
    "    )):\n",
    "        writer.writerow([epoch, total, pde, boundary, initial, norm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# use float64 for higher precision in PDEs\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "DTYPE = tf.float64\n",
    "\n",
    "# declare inputs\n",
    "# w_min, w_max = tf.constant(0.75, dtype=DTYPE), tf.constant(2.0, dtype=DTYPE)\n",
    "w_choice = tf.constant(1.0, dtype=DTYPE)\n",
    "pi = tf.constant(np.pi, dtype=DTYPE)\n",
    "x_min, x_max = -pi, pi\n",
    "t_min, t_max = tf.constant(0.0, dtype=DTYPE), pi/2.0\n",
    "\n",
    "# scale inputs\n",
    "@tf.function\n",
    "def scale_inputs(input, input_min, input_max):\n",
    "    return 2.0 * (input - input_min) / (input_max - input_min) - 1.0\n",
    "\n",
    "# # renormalization\n",
    "# @tf.function\n",
    "# def normalization(input_min, input_max, N, psi_Re, psi_Im):\n",
    "#     dx = (input_max - input_min) / tf.cast(N, dtype=DTYPE)\n",
    "#     norm = tf.math.sqrt(tf.reduce_sum(psi_Re**2 + psi_Im**2) * dx)\n",
    "#     return (psi_Re/norm), (psi_Im/norm)\n",
    "\n",
    "# initial condition\n",
    "@tf.function\n",
    "def psi_init(x, w, pi):\n",
    "    u_0 = (w/pi)**0.25 * tf.math.exp(-0.5*w*x**2)\n",
    "    u_1 = (w/pi)**0.25 * tf.math.exp(-0.5*w*x**2) * x * tf.math.sqrt(2.0*w)\n",
    "    return (1.0/np.sqrt(2.0)) * (u_0 + u_1)\n",
    "\n",
    "# data generation\n",
    "N_f, N_b, N_i, N_grid = 5000, 1000, 1000, 200\n",
    "\n",
    "# interior points\n",
    "x_f = tf.random.uniform((N_f, 1), x_min, x_max, dtype=DTYPE)\n",
    "t_f = tf.random.uniform((N_f, 1), t_min, t_max, dtype=DTYPE)\n",
    "x_fs = scale_inputs(x_f, x_min, x_max)  # scaled\n",
    "t_fs = scale_inputs(t_f, t_min, t_max)  # scaled\n",
    "\n",
    "# boundary points\n",
    "x_b = tf.concat([tf.ones((N_b//2, 1), dtype=DTYPE) * x_min,\n",
    "                 tf.ones((N_b//2, 1), dtype=DTYPE) * x_max], axis=0)\n",
    "t_b = tf.random.uniform((N_b, 1), t_min, t_max, dtype=DTYPE)\n",
    "x_bs = scale_inputs(x_b, x_min, x_max)  # scaled\n",
    "t_bs = scale_inputs(t_b, t_min, t_max)  # scaled\n",
    "\n",
    "# initial points\n",
    "x_i = tf.random.uniform((N_i, 1), x_min, x_max, dtype=DTYPE)\n",
    "t_i = tf.zeros_like(x_i, dtype=DTYPE)\n",
    "x_is = scale_inputs(x_i, x_min, x_max)  # scaled\n",
    "t_is = scale_inputs(t_i, t_min, t_max)  # scaled\n",
    "\n",
    "psi_i = tf.cast(psi_init(x_i, w_choice, pi), tf.complex128)\n",
    "\n",
    "# grid for normalization penalty\n",
    "x_n = tf.linspace(x_min, x_max, N_grid)[:, None]\n",
    "x_ns = scale_inputs(x_n, x_min, x_max)\n",
    "\n",
    "# verify normalization\n",
    "dx_i = (x_max - x_min) / N_i\n",
    "norm_check = tf.reduce_sum(tf.math.abs(psi_i)**2) * dx_i\n",
    "print(f'Normalization check for the initial condition: $|\\Psi(x,0)|**2={norm_check}$')\n",
    "\n",
    "# Fourier feature layer\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class FourierFeatureLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, mapping_size=512, scale=10.0, **kwargs):\n",
    "        super(FourierFeatureLayer, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.mapping_size = mapping_size\n",
    "        self.scale = scale\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.B = self.add_weight(name=\"B\",\n",
    "                                 shape=[self.input_dim, self.mapping_size],\n",
    "                                 initializer=tf.random_normal_initializer(stddev=self.scale),\n",
    "                                 trainable=False)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x_proj = tf.matmul(x, self.B)\n",
    "        return tf.concat([tf.math.sin(x_proj), tf.math.cos(x_proj)], axis=-1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"mapping_size\": self.mapping_size,\n",
    "            \"scale\": self.scale\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "# Define the Neural Network\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, input_dim=2, mapping_size=512, scale=10.0, **kwargs):\n",
    "        super(PINN, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.mapping_size = mapping_size\n",
    "        self.scale = scale\n",
    "\n",
    "        self.fourier = FourierFeatureLayer(input_dim=input_dim, mapping_size=mapping_size, scale=scale)\n",
    "        initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(512, activation='tanh', kernel_initializer=initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(512, activation='tanh', kernel_initializer=initializer)\n",
    "        self.dense3 = tf.keras.layers.Dense(512, activation='tanh', kernel_initializer=initializer)\n",
    "        self.dense4 = tf.keras.layers.Dense(512, activation='tanh', kernel_initializer=initializer)\n",
    "        self.out = tf.keras.layers.Dense(2, activation='linear', kernel_initializer=initializer) # u_Re, u_Im\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.fourier(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"mapping_size\": self.mapping_size,\n",
    "            \"scale\": self.scale\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('best_model_w1_s1.keras', custom_objects={'PINN': PINN})\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "loaded_model.summary()\n",
    "\n",
    "################## Plot: Contour Plot ###############################\n",
    "\n",
    "# Contour Plot\n",
    "N = 100\n",
    "x_plot = np.linspace(x_min, x_max, N)[:,None]\n",
    "t_plot = np.linspace(t_min, t_max, N)[:,None]\n",
    "x_plot_scaled = scale_inputs(x_plot, x_min, x_max)\n",
    "t_plot_scaled = scale_inputs(t_plot, t_min, t_max)\n",
    "X_grid, T_grid = np.meshgrid(x_plot, t_plot)  # t and x grid\n",
    "\n",
    "x_plot_in = tf.convert_to_tensor(x_plot_scaled, dtype=DTYPE)\n",
    "t_plot_in = tf.convert_to_tensor(t_plot_scaled, dtype=DTYPE)\n",
    "\n",
    "density_interior = []\n",
    "\n",
    "count = 0\n",
    "for t_val in tf.unstack(t_plot_in, axis=0):\n",
    "    t_in = tf.ones_like(x_plot_in) * t_val\n",
    "    input = tf.concat([x_plot_in, t_in], axis=1)\n",
    "    uv = loaded_model(input)\n",
    "    u, v = uv[:, 0], uv[:, 1]\n",
    "    density = u**2 + v**2\n",
    "    # psi_Re, psi_Im = normalization(x_min, x_max, N, u, v)\n",
    "    # density = psi_Re**2 + psi_Im**2\n",
    "    density_interior.append(density)\n",
    "\n",
    "# plt.figure(figsize=(8, 5))\n",
    "contour = plt.contourf(X_grid, T_grid, density_interior, levels=100, cmap=\"viridis\")\n",
    "plt.colorbar(contour, label=r'$|\\Psi(x,t)|^2$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$t$')\n",
    "plt.title(f'Predicted $|\\Psi(x,t)|^2$ with $\\omega={w_choice}$')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "################## Plot: Initial Condition ###############################\n",
    "\n",
    "X = tf.concat([x_is, t_is], axis=1)\n",
    "uv = loaded_model(X)\n",
    "u, v = uv[:, 0:1], uv[:, 1:2]\n",
    "# psi_Re, psi_Im = normalization(x_min, x_max, N_i, u, v)\n",
    "# density_initial = psi_Re**2 + psi_Im**2\n",
    "\n",
    "plt.scatter(x_i, u**2 + v**2, label=r'Predicted $|\\Psi(x,0)|^2$')\n",
    "# plt.scatter(x_i, density_initial, label=r'Normalized Predicted $|\\Psi(x,0)|^2$')\n",
    "plt.scatter(x_i, tf.math.abs(psi_i)**2, label=r'Expected $|\\Psi(x,0)|^2$')\n",
    "plt.xlabel('x')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "################## Plot: Boundary Condition ###############################\n",
    "\n",
    "X = tf.concat([x_bs, t_bs], axis=1)\n",
    "uv = loaded_model(X)\n",
    "u, v = uv[:, 0:1], uv[:, 1:2]\n",
    "# psi_Re, psi_Im = normalization(x_min, x_max, N_b, u, v)\n",
    "# density_boundary = psi_Re**2 + psi_Im**2\n",
    "\n",
    "plt.scatter(t_b, u**2 + v**2, label=r'Predicted $|\\Psi(x_b,t)|^2$')\n",
    "# plt.scatter(t_b, density_boundary, label=r'Normalized Predicted $|\\Psi(x_b,t)|^2$')\n",
    "plt.xlabel('t')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fffbf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true = np.linspace(x_min, x_max, 100)\n",
    "\n",
    "# Compute theoretical |Psi(x,t)|^2 as before\n",
    "def U_0(x, t):\n",
    "    w = 1.0\n",
    "    return np.exp(-1j * 0.5 * w * t) * (w/np.pi)**0.25 * np.exp(-w * x**2 / 2.0)\n",
    "\n",
    "def U_1(x, t):\n",
    "    w = 1.0\n",
    "    return np.exp(-1j * 1.5 * w * t) * (w/np.pi)**0.25 * np.exp(-w * x**2 / 2.0) * x * np.sqrt(2.0*w)\n",
    "\n",
    "# Define time steps\n",
    "t_test = [0, np.pi/2.0, np.pi, 2.0*np.pi]\n",
    "# t_test = [0, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, len(t_test), figsize=(15, 5), sharey=True)\n",
    "\n",
    "for i, t_val in enumerate(t_test):\n",
    "    t_in = tf.ones_like(x_is) * t_val\n",
    "    t_ins = scale_inputs(t_in, t_min, t_max)\n",
    "    X = tf.concat([x_is, t_ins], axis=1)\n",
    "    uv = loaded_model(X)\n",
    "    u, v = uv[:, 0:1], uv[:, 1:2]\n",
    "    # psi_Re, psi_Im = normalization(x_min, x_max, N_b, u, v)\n",
    "    # density = psi_Re**2 + psi_Im**2\n",
    "\n",
    "    t_true = np.ones_like(x_true) * t_val\n",
    "    psi_true = (1.0 / np.sqrt(2.0)) * (U_0(x_true, t_true) + U_1(x_true, t_true))\n",
    "    density_true = np.abs(psi_true)**2\n",
    "\n",
    "    ax = axs[i]\n",
    "    # ax.scatter(x_i, density, s=10, label=\"Normalized\", alpha=0.7)\n",
    "    ax.scatter(x_i, u**2 + v**2, s=10, label=\"Raw\", alpha=0.5)\n",
    "    ax.scatter(x_true, density_true, s=10, label=\"True\", alpha=0.6)\n",
    "    ax.set_title(f\"$t = {t_val:.2f}$\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r\"$|\\Psi(x,t)|^2$\")\n",
    "    ax.grid(True)\n",
    "\n",
    "# Add a single legend and adjust layout\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', ncol=2)\n",
    "fig.suptitle(\"Predicted $|\\Psi(x,t)|^2$ at Different Time Steps\", fontsize=14)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
